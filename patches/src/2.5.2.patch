diff -urN -x __pycache__ -x '*dist-info' worker/venv/lib/python3.10/site-packages/crynux_worker/__init__.py new_worker/venv/lib/python3.10/site-packages/crynux_worker/__init__.py
--- worker/venv/lib/python3.10/site-packages/crynux_worker/__init__.py	2025-07-07 18:26:49.379332037 +0800
+++ new_worker/venv/lib/python3.10/site-packages/crynux_worker/__init__.py	2025-07-07 18:24:48.471817199 +0800
@@ -1,4 +1,4 @@
-__version__ = "2.5.1"
+__version__ = "2.5.2"
 
 def version():
     return __version__
diff -urN -x __pycache__ -x '*dist-info' worker/venv/lib/python3.10/site-packages/crynux_worker/task/runner.py new_worker/venv/lib/python3.10/site-packages/crynux_worker/task/runner.py
--- worker/venv/lib/python3.10/site-packages/crynux_worker/task/runner.py	2025-07-07 18:26:49.383554328 +0800
+++ new_worker/venv/lib/python3.10/site-packages/crynux_worker/task/runner.py	2025-07-07 18:24:48.475986758 +0800
@@ -2,7 +2,9 @@
 import logging
 import os
 from abc import ABC, abstractmethod
-from typing import Literal
+import shutil
+import tempfile
+from typing import Dict, Literal
 
 from gpt_task.config import Config as GPTConfig
 from sd_task.config import Config as SDConfig
@@ -10,9 +12,9 @@
 from crynux_worker.model import TaskType
 from crynux_worker.model_cache import ModelCache
 
-
 _logger = logging.getLogger(__name__)
 
+
 class TaskRunner(ABC):
     @abstractmethod
     def download_model(
@@ -61,10 +63,8 @@
             from diffusers import ControlNetModel
             from diffusers.utils import SAFETENSORS_WEIGHTS_NAME, WEIGHTS_NAME
             from sd_task.download_model import (
-                check_and_download_hf_model,
-                check_and_download_hf_pipeline,
-                check_and_download_model_by_name,
-            )
+                check_and_download_hf_model, check_and_download_hf_pipeline,
+                check_and_download_model_by_name)
 
             if model_type == "base":
                 check_and_download_hf_pipeline(
@@ -109,7 +109,7 @@
     ):
         if task_type == TaskType.SD:
             from sd_task.task_args.inference_task import InferenceTaskArgs
-            from sd_task.task_runner import run_inference_task
+            from sd_task.task_runner.inference_task import run_inference_task
 
             args = InferenceTaskArgs.model_validate_json(task_args)
             imgs = run_inference_task(args, model_cache=model_cache, config=sd_config)
@@ -128,7 +128,8 @@
                 json.dump(resp, f)
         elif task_type == TaskType.SD_FT_LORA:
             from sd_task.task_args import FinetuneLoraTaskArgs
-            from sd_task.task_runner import run_finetune_lora_task
+            from sd_task.task_runner.finetune_task import \
+                run_finetune_lora_task
 
             args = FinetuneLoraTaskArgs.model_validate_json(task_args)
             run_finetune_lora_task(args, output_dir=output_dir, config=sd_config)
@@ -144,7 +145,9 @@
         sd_config: SDConfig,
         gpt_config: GPTConfig,
     ):
-        _logger.info(f"Successfully download {task_type} {model_type} model: {model_name}")
+        _logger.info(
+            f"Successfully download {task_type} {model_type} model: {model_name}"
+        )
 
     def inference(
         self,
@@ -162,8 +165,7 @@
             with open(os.path.join(output_dir, "0.png"), mode="wb") as f:
                 f.write(content)
         elif task_type == TaskType.LLM:
-            resp = """
-            {
+            resp = {
                 "model": "gpt2",
                 "choices": [
                     {
@@ -176,12 +178,57 @@
                         "index": 0,
                     }
                 ],
-                "usage": {"prompt_tokens": 11, "completion_tokens": 30, "total_tokens": 41},
-            }"""
+                "usage": {
+                    "prompt_tokens": 11,
+                    "completion_tokens": 30,
+                    "total_tokens": 41,
+                },
+            }
             with open(
                 os.path.join(output_dir, "0.json"), mode="w", encoding="utf-8"
             ) as f:
-                f.write(resp)
+                json.dump(resp, f)
         elif task_type == TaskType.SD_FT_LORA:
-            os.mkdir(os.path.join(output_dir, "validation"))
-            os.mkdir(os.path.join(output_dir, "checkpoint"))
+            from sd_task.task_args import FinetuneLoraTaskArgs
+            args = FinetuneLoraTaskArgs.model_validate_json(task_args)
+            if args.checkpoint is not None:
+                assert os.path.exists(args.checkpoint)
+                assert os.path.isdir(args.checkpoint)
+            validation_dir = os.path.join(output_dir, "validation")
+            checkpoint_dir = os.path.join(output_dir, "checkpoint")
+            os.makedirs(validation_dir, exist_ok=True)
+            os.makedirs(checkpoint_dir, exist_ok=True)
+            with open(os.path.join(validation_dir, "0.png"), mode="wb") as f:
+                content = bytes.fromhex(
+                    "89504e470d0a1a0a0000000d4948445200000008000000080800000000e164e1570000000c49444154789c6360a00e0000004800012eb83c7e0000000049454e44ae426082"
+                )
+                f.write(content)
+
+            global_step = 0
+            global_epoch = 0
+            finish = False
+            if args.checkpoint is not None:
+                if os.path.exists(os.path.join(args.checkpoint, "global_step.txt")):
+                    with open(os.path.join(args.checkpoint, "global_step.txt"), mode="r", encoding="utf-8") as f:
+                        global_step = int(f.read().strip())
+                if os.path.exists(os.path.join(args.checkpoint, "global_epoch.txt")):
+                    with open(os.path.join(args.checkpoint, "global_epoch.txt"), mode="r", encoding="utf-8") as f:
+                        global_epoch = int(f.read().strip())
+            if args.train_args.max_train_steps is not None and args.train_args.num_train_steps is not None and args.train_args.max_train_steps > 0:
+                global_step += args.train_args.num_train_steps
+                with open(os.path.join(checkpoint_dir, "global_step.txt"), mode="w", encoding="utf-8") as f:
+                    f.write(str(global_step))
+                if global_step >= args.train_args.max_train_steps:
+                    finish = True
+            elif args.train_args.max_train_epochs > 0 and args.train_args.num_train_epochs > 0:
+                global_epoch += args.train_args.num_train_epochs
+                with open(os.path.join(checkpoint_dir, "global_epoch.txt"), mode="w", encoding="utf-8") as f:
+                    f.write(str(global_epoch))
+                if global_epoch >= args.train_args.max_train_epochs:
+                    finish = True  
+            else:
+                raise ValueError("max_train_steps or max_train_epochs must be set")
+
+            if finish:
+                with open(os.path.join(checkpoint_dir, "FINISH"), mode="w", encoding="utf-8") as f:
+                    f.write("")
diff -urN -x __pycache__ -x '*dist-info' worker/venv/lib/python3.10/site-packages/sd_task/__init__.py new_worker/venv/lib/python3.10/site-packages/sd_task/__init__.py
--- worker/venv/lib/python3.10/site-packages/sd_task/__init__.py	2025-07-07 18:26:32.179630200 +0800
+++ new_worker/venv/lib/python3.10/site-packages/sd_task/__init__.py	2025-07-07 18:23:05.923403457 +0800
@@ -1,4 +1,4 @@
-__version__ = "2.5.0"
+__version__ = "2.6.1"
 
 def version():
     return __version__
diff -urN -x __pycache__ -x '*dist-info' worker/venv/lib/python3.10/site-packages/sd_task/task_args/finetune_task/task_args.py new_worker/venv/lib/python3.10/site-packages/sd_task/task_args/finetune_task/task_args.py
--- worker/venv/lib/python3.10/site-packages/sd_task/task_args/finetune_task/task_args.py	2025-07-07 18:26:32.179630200 +0800
+++ new_worker/venv/lib/python3.10/site-packages/sd_task/task_args/finetune_task/task_args.py	2025-07-07 18:23:05.927568483 +0800
@@ -1,7 +1,7 @@
 from typing import Annotated, Literal, List
 
 from annotated_types import Ge, Gt, MinLen, Le
-from pydantic import BaseModel
+from pydantic import BaseModel, HttpUrl
 
 from ..version import VersionString
 
@@ -43,7 +43,8 @@
 
 
 class DatasetArgs(BaseModel):
-    name: Annotated[str, MinLen(1)]
+    url: HttpUrl | None = None
+    name: Annotated[str, MinLen(1)] | None = None
     config_name: str | None = None
     image_column: str = "image"
     caption_column: str = "text"
diff -urN -x __pycache__ -x '*dist-info' worker/venv/lib/python3.10/site-packages/sd_task/task_runner/finetune_task/download_url_dataset.py new_worker/venv/lib/python3.10/site-packages/sd_task/task_runner/finetune_task/download_url_dataset.py
--- worker/venv/lib/python3.10/site-packages/sd_task/task_runner/finetune_task/download_url_dataset.py	1970-01-01 08:00:00.000000000 +0800
+++ new_worker/venv/lib/python3.10/site-packages/sd_task/task_runner/finetune_task/download_url_dataset.py	2025-07-07 18:23:05.931733509 +0800
@@ -0,0 +1,377 @@
+import requests
+import os
+import zipfile
+import tarfile
+from urllib.parse import urlparse, unquote
+
+import logging
+
+_logger = logging.getLogger(__name__)
+
+
+def _detect_file_type_by_magic_number(file_path: str) -> str:
+    """
+    Detect file type by magic number (file signature)
+    Returns file extension (including dot), returns empty string if unknown type
+    """
+    try:
+        with open(file_path, 'rb') as f:
+            # Read file header bytes
+            header = f.read(8)
+            
+            if len(header) < 2:
+                return ""
+            
+            # ZIP file: PK\x03\x04
+            if header.startswith(b'PK\x03\x04'):
+                return ".zip"
+            
+            # GZIP file: \x1f\x8b
+            if header.startswith(b'\x1f\x8b'):
+                # Check if it's a tar.gz by examining the decompressed content
+                if _is_tar_inside_gzip(file_path):
+                    return ".tar.gz"
+                return ".gz"
+            
+            # BZIP2 file: BZ
+            if header.startswith(b'BZ'):
+                # Check if it's a tar.bz2 by examining the decompressed content
+                if _is_tar_inside_bzip2(file_path):
+                    return ".tar.bz2"
+                return ".bz2"
+            
+            # XZ file: \xfd7zXZ\x00
+            if header.startswith(b'\xfd7zXZ\x00'):
+                # Check if it's a tar.xz by examining the decompressed content
+                if _is_tar_inside_xz(file_path):
+                    return ".tar.xz"
+                return ".xz"
+            
+            # TAR file: ustar (GNU tar) or POSIX tar
+            if len(header) >= 262:  # TAR header at offset 257 bytes
+                f.seek(257)
+                tar_header = f.read(8)
+                if tar_header.startswith(b'ustar\x00') or tar_header.startswith(b'ustar '):
+                    return ".tar"
+            
+            # 7Z file: 7z\xbc\xaf\x27\x1c
+            if header.startswith(b'7z\xbc\xaf\x27\x1c'):
+                return ".7z"
+            
+            # RAR file: Rar!\x1a\x07
+            if header.startswith(b'Rar!\x1a\x07'):
+                return ".rar"
+            
+            return ""
+            
+    except Exception as e:
+        _logger.warning(f"Failed to detect file type by magic number: {e}")
+        return ""
+
+
+def _is_tar_inside_gzip(file_path: str) -> bool:
+    """Check if the content inside a gzip file is a tar file."""
+    try:
+        import gzip
+        with gzip.open(file_path, 'rb') as gz_file:
+            # Read the first 512 bytes (tar header size)
+            header = gz_file.read(512)
+            if len(header) < 262:
+                return False
+            
+            # Check for tar magic number at offset 257
+            tar_magic = header[257:265]
+            return tar_magic.startswith(b'ustar\x00') or tar_magic.startswith(b'ustar ')
+    except Exception as e:
+        _logger.debug(f"Failed to check tar inside gzip: {e}")
+        return False
+
+
+def _is_tar_inside_bzip2(file_path: str) -> bool:
+    """Check if the content inside a bzip2 file is a tar file."""
+    try:
+        import bz2
+        with bz2.open(file_path, 'rb') as bz_file:
+            # Read the first 512 bytes (tar header size)
+            header = bz_file.read(512)
+            if len(header) < 262:
+                return False
+            
+            # Check for tar magic number at offset 257
+            tar_magic = header[257:265]
+            return tar_magic.startswith(b'ustar\x00') or tar_magic.startswith(b'ustar ')
+    except Exception as e:
+        _logger.debug(f"Failed to check tar inside bzip2: {e}")
+        return False
+
+
+def _is_tar_inside_xz(file_path: str) -> bool:
+    """Check if the content inside an xz file is a tar file."""
+    try:
+        import lzma
+        with lzma.open(file_path, 'rb') as xz_file:
+            # Read the first 512 bytes (tar header size)
+            header = xz_file.read(512)
+            if len(header) < 262:
+                return False
+            
+            # Check for tar magic number at offset 257
+            tar_magic = header[257:265]
+            return tar_magic.startswith(b'ustar\x00') or tar_magic.startswith(b'ustar ')
+    except Exception as e:
+        _logger.debug(f"Failed to check tar inside xz: {e}")
+        return False
+
+
+def _get_file_extension_from_content_type(content_type: str) -> str:
+    """
+    Get file extension from Content-Type header
+    Note: This function cannot detect compound formats like .tar.gz, .tar.xz, .tar.bz2
+    as Content-Type headers typically only indicate the outer compression format.
+    """
+    if not content_type:
+        return ""
+    
+    content_type_lower = content_type.lower()
+    
+    if "zip" in content_type_lower:
+        return ".zip"
+    elif "tar" in content_type_lower:
+        return ".tar"
+    elif "gzip" in content_type_lower or "gz" in content_type_lower:
+        return ".gz"
+    elif "bzip2" in content_type_lower or "bz2" in content_type_lower:
+        return ".bz2"
+    elif "xz" in content_type_lower:
+        return ".xz"
+    elif "7z" in content_type_lower:
+        return ".7z"
+    elif "rar" in content_type_lower:
+        return ".rar"
+    
+    return ""
+
+
+def download_dataset_from_url(url: str, save_dir: str = ".") -> str:
+    tmp_file_path: str | None = None
+    try:
+        response = requests.head(url, allow_redirects=True)
+        response.raise_for_status()
+
+        filename: str | None = None
+        content_disposition = response.headers.get("Content-Disposition")
+        if content_disposition:
+            import re
+
+            filename_match = re.search(
+                r'filename[^;=\n]*=(([\'"]).*?\2|[^;\n]*)', content_disposition
+            )
+            if filename_match:
+                filename = filename_match.group(1).strip("\"'")
+
+        if not filename:
+            parsed_url = urlparse(url)
+            filename = os.path.basename(parsed_url.path)
+            if not filename:
+                filename = "dataset"
+
+        filename = unquote(filename)
+        filename = "".join(c for c in filename if c.isalnum() or c in "._- ")
+
+        file_path = os.path.join(save_dir, filename)
+        tmp_file_path = file_path + ".tmp"
+
+        if os.path.exists(file_path):
+            _logger.info(f"dataset already exists at {file_path}")
+            return _handle_existing_file(file_path, save_dir)
+
+        _logger.info(f"start downloading dataset to {file_path}")
+        response = requests.get(url, stream=True)
+        response.raise_for_status()
+
+        total_size = int(response.headers.get("content-length", 0))
+
+        downloaded = 0
+        with open(tmp_file_path, "wb") as f:
+            for chunk in response.iter_content(chunk_size=8192):
+                if chunk:
+                    f.write(chunk)
+                    downloaded += len(chunk)
+                    if total_size > 0:
+                        progress = (downloaded / total_size) * 100
+                        _logger.info(
+                            f"downloading dataset to {file_path} {downloaded / 1024} kb {progress:.1f}%"
+                        )
+                    else:
+                        _logger.info(
+                            f"downloading dataset to {file_path} {downloaded / 1024} kb"
+                        )
+
+        # After download, check file type and add appropriate extension
+        if "." not in filename:
+            # First try to detect file type by magic number
+            detected_ext = _detect_file_type_by_magic_number(tmp_file_path)
+            
+            if not detected_ext:
+                # If magic number detection fails, try Content-Type
+                content_type = response.headers.get("Content-Type")
+                if content_type:
+                    detected_ext = _get_file_extension_from_content_type(content_type)
+            
+            if detected_ext:
+                filename += detected_ext
+                file_path = os.path.join(save_dir, filename)
+                _logger.info(f"detected file type: {detected_ext}, renamed to {file_path}")
+
+        os.rename(tmp_file_path, file_path)
+
+        _logger.info(f"downloaded dataset to {file_path}")
+        return _handle_downloaded_file(file_path, save_dir)
+
+    except Exception as e:
+        _logger.error(f"failed to download dataset from {url}: {e}")
+        raise e
+    finally:
+        if tmp_file_path and os.path.exists(tmp_file_path):
+            os.remove(tmp_file_path)
+
+
+def _handle_existing_file(file_path: str, save_dir: str) -> str:
+    """Handle existing file - check if it's compressed and extract if needed."""
+    if os.path.isdir(file_path):
+        return file_path
+    else:
+        return _extract_if_compressed(file_path, save_dir)
+
+
+def _handle_downloaded_file(file_path: str, save_dir: str) -> str:
+    """Handle downloaded file - check if it's compressed and extract if needed."""
+    return _extract_if_compressed(file_path, save_dir)
+
+
+def _extract_if_compressed(file_path: str, save_dir: str) -> str:
+    """Extract compressed file if it's a supported format, otherwise return the file path."""
+    filename = os.path.basename(file_path)
+    name_without_ext = os.path.splitext(filename)[0]
+    
+    # Handle double extensions like .tar.gz
+    if name_without_ext.endswith('.tar'):
+        name_without_ext = os.path.splitext(name_without_ext)[0]
+    
+    extract_dir = os.path.join(save_dir, name_without_ext)
+    
+    # Check if it's a zip file
+    if filename.lower().endswith('.zip'):
+        if os.path.exists(extract_dir):
+            _logger.info(f"extracted dataset already exists at {extract_dir}")
+            return extract_dir
+        
+        _logger.info(f"extracting zip file {file_path} to {extract_dir}")
+        with zipfile.ZipFile(file_path, 'r') as zip_ref:
+            zip_ref.extractall(extract_dir)
+        _logger.info(f"extracted dataset to {extract_dir}")
+        os.remove(file_path)
+        return extract_dir
+    
+    # Check if it's a tar file (including .tar.gz, .tar.bz2, etc.)
+    elif filename.lower().endswith(('.tar', '.tar.gz', '.tar.bz2', '.tar.xz')):
+        if os.path.exists(extract_dir):
+            _logger.info(f"extracted dataset already exists at {extract_dir}")
+            return extract_dir
+        
+        _logger.info(f"extracting tar file {file_path} to {extract_dir}")
+        mode = 'r'
+        if filename.lower().endswith('.gz'):
+            mode = 'r:gz'
+        elif filename.lower().endswith('.bz2'):
+            mode = 'r:bz2'
+        elif filename.lower().endswith('.xz'):
+            mode = 'r:xz'
+        
+        with tarfile.open(file_path, mode) as tar_ref:
+            tar_ref.extractall(extract_dir)
+        _logger.info(f"extracted dataset to {extract_dir}")
+        os.remove(file_path)
+        return extract_dir
+    
+    # Check if it's a gzip file (not tar.gz)
+    elif filename.lower().endswith('.gz'):
+        if os.path.exists(extract_dir):
+            _logger.info(f"extracted dataset already exists at {extract_dir}")
+            return extract_dir
+        
+        _logger.info(f"extracting gzip file {file_path} to {extract_dir}")
+        import gzip
+        import shutil
+        
+        # Create extract directory if it doesn't exist
+        os.makedirs(extract_dir, exist_ok=True)
+        
+        # Extract the gzip file to the extract directory
+        extracted_file_path = os.path.join(extract_dir, name_without_ext)
+        with gzip.open(file_path, 'rb') as gz_file:
+            with open(extracted_file_path, 'wb') as out_file:
+                shutil.copyfileobj(gz_file, out_file)
+        
+        _logger.info(f"extracted dataset to {extracted_file_path}")
+        os.remove(file_path)
+        return extracted_file_path
+    
+    # Check if it's a bzip2 file (not tar.bz2)
+    elif filename.lower().endswith('.bz2'):
+        if os.path.exists(extract_dir):
+            _logger.info(f"extracted dataset already exists at {extract_dir}")
+            return extract_dir
+        
+        _logger.info(f"extracting bzip2 file {file_path} to {extract_dir}")
+        import bz2
+        import shutil
+        
+        # Create extract directory if it doesn't exist
+        os.makedirs(extract_dir, exist_ok=True)
+        
+        # Extract the bzip2 file to the extract directory
+        extracted_file_path = os.path.join(extract_dir, name_without_ext)
+        with bz2.open(file_path, 'rb') as bz_file:
+            with open(extracted_file_path, 'wb') as out_file:
+                shutil.copyfileobj(bz_file, out_file)
+        
+        _logger.info(f"extracted dataset to {extracted_file_path}")
+        os.remove(file_path)
+        return extracted_file_path
+    
+    # Check if it's an xz file (not tar.xz)
+    elif filename.lower().endswith('.xz'):
+        if os.path.exists(extract_dir):
+            _logger.info(f"extracted dataset already exists at {extract_dir}")
+            return extract_dir
+        
+        _logger.info(f"extracting xz file {file_path} to {extract_dir}")
+        import lzma
+        import shutil
+        
+        # Create extract directory if it doesn't exist
+        os.makedirs(extract_dir, exist_ok=True)
+        
+        # Extract the xz file to the extract directory
+        extracted_file_path = os.path.join(extract_dir, name_without_ext)
+        with lzma.open(file_path, 'rb') as xz_file:
+            with open(extracted_file_path, 'wb') as out_file:
+                shutil.copyfileobj(xz_file, out_file)
+        
+        _logger.info(f"extracted dataset to {extracted_file_path}")
+        os.remove(file_path)
+        return extracted_file_path
+    
+    # If filename has no extension, try to detect file type by magic number
+    elif "." not in filename:
+        detected_ext = _detect_file_type_by_magic_number(file_path)
+        if detected_ext:
+            _logger.info(f"detected file type by magic number: {detected_ext}")
+            # Recursively call self to handle the detected file type
+            new_file_path = file_path + detected_ext
+            os.rename(file_path, new_file_path)
+            return _extract_if_compressed(new_file_path, save_dir)
+    
+    # Not a compressed file, return the original file path
+    return file_path
diff -urN -x __pycache__ -x '*dist-info' worker/venv/lib/python3.10/site-packages/sd_task/task_runner/finetune_task/finetune_lora.py new_worker/venv/lib/python3.10/site-packages/sd_task/task_runner/finetune_task/finetune_lora.py
--- worker/venv/lib/python3.10/site-packages/sd_task/task_runner/finetune_task/finetune_lora.py	2025-07-07 18:26:32.183799130 +0800
+++ new_worker/venv/lib/python3.10/site-packages/sd_task/task_runner/finetune_task/finetune_lora.py	2025-07-07 18:23:05.931733509 +0800
@@ -1,46 +1,43 @@
+import hashlib
+import json
 import math
 import os
 import random
-import json
-import hashlib
 from contextlib import nullcontext
-from typing import cast, List
+from typing import List, cast
 
 import numpy as np
 import torch
 import torch.nn.functional as F
 import torch.utils.checkpoint
-from torch.utils.data import DataLoader
 from accelerate import Accelerator
 from accelerate.logging import get_logger
 from accelerate.utils import set_seed
-from datasets import load_dataset, DatasetDict
+from datasets import DatasetDict, load_dataset
+from diffusers import (AutoencoderKL, DDPMScheduler, DiffusionPipeline,
+                       StableDiffusionPipeline, UNet2DConditionModel)
+from diffusers.optimization import get_scheduler
+from diffusers.training_utils import cast_training_params, compute_snr
+from diffusers.utils import convert_state_dict_to_diffusers
+from diffusers.utils.torch_utils import is_compiled_module
 from peft import LoraConfig
 from peft.utils import get_peft_model_state_dict
 from PIL import Image
+from torch.utils.data import DataLoader
 from torchvision import transforms
 from tqdm.auto import tqdm
 from transformers import CLIPTextModel, CLIPTokenizer
 
-from diffusers import (
-    AutoencoderKL,
-    DDPMScheduler,
-    DiffusionPipeline,
-    StableDiffusionPipeline,
-    UNet2DConditionModel,
-)
-from diffusers.optimization import get_scheduler
-from diffusers.training_utils import cast_training_params, compute_snr
-from diffusers.utils import convert_state_dict_to_diffusers
-from diffusers.utils.torch_utils import is_compiled_module
-
+from sd_task import utils
+from sd_task.cache import ModelCache
 from sd_task.config import Config, get_config
 from sd_task.task_args import FinetuneLoraTaskArgs
-from sd_task.cache import ModelCache
-from sd_task import utils
+
+from .download_url_dataset import download_dataset_from_url
 
 _logger = get_logger(__name__, log_level="INFO")
 
+
 def generate_model_key(args: FinetuneLoraTaskArgs):
     model_args = {
         "model_name": args.model.name,
@@ -50,7 +47,9 @@
     if args.model.variant is not None:
         model_args["model_variant"] = args.model.variant
 
-    model_args_str = json.dumps(model_args, ensure_ascii=False, separators=(",", ":"), sort_keys=True)
+    model_args_str = json.dumps(
+        model_args, ensure_ascii=False, separators=(",", ":"), sort_keys=True
+    )
     key = hashlib.md5(model_args_str.encode("utf-8")).hexdigest()
     return key
 
@@ -178,7 +177,9 @@
 
     if model_cache is not None:
         key = generate_model_key(args)
-        noise_scheduler, tokenizer, text_encoder, vae, unet, pipeline = model_cache.load(key, load_model)
+        noise_scheduler, tokenizer, text_encoder, vae, unet, pipeline = (
+            model_cache.load(key, load_model)
+        )
     else:
         noise_scheduler, tokenizer, text_encoder, vae, unet, pipeline = load_model()
 
@@ -201,12 +202,24 @@
         eps=train_args.adam_args.epsilon,
     )
 
-    dataset = load_dataset(
-        args.dataset.name,
-        args.dataset.config_name,
-        cache_dir=cache_dir,
-    )
-    dataset = cast(DatasetDict, dataset)
+    if args.dataset.name is not None:
+        dataset = load_dataset(
+            args.dataset.name,
+            args.dataset.config_name,
+            cache_dir=cache_dir,
+        )
+        dataset = cast(DatasetDict, dataset)
+    elif args.dataset.url is not None:
+        cache_dir = config.data_dir.models.external
+        url = str(args.dataset.url)
+        dirname = hashlib.md5(url.encode("utf-8")).hexdigest()
+        dataset_dir = os.path.join(cache_dir, dirname)
+        os.makedirs(dataset_dir, exist_ok=True)
+        dataset_path = download_dataset_from_url(url, dataset_dir)
+        dataset = load_dataset(dataset_path, args.dataset.config_name)
+        dataset = cast(DatasetDict, dataset)
+    else:
+        raise ValueError("Either dataset.name or dataset.url must be provided")
 
     column_names = dataset["train"].column_names
     image_column = args.dataset.image_column
diff -urN -x __pycache__ -x '*dist-info' worker/venv/lib/python3.10/site-packages/sd_task/task_runner/inference_task/inference_task.py new_worker/venv/lib/python3.10/site-packages/sd_task/task_runner/inference_task/inference_task.py
--- worker/venv/lib/python3.10/site-packages/sd_task/task_runner/inference_task/inference_task.py	2025-07-07 18:26:32.183799130 +0800
+++ new_worker/venv/lib/python3.10/site-packages/sd_task/task_runner/inference_task/inference_task.py	2025-07-07 18:23:05.931733509 +0800
@@ -212,11 +212,6 @@
     if config is None:
         config = get_config()
 
-    # Get GPU information
-    gpu_info = utils.get_gpu_info()
-    if gpu_info:
-        log(f"GPU: {gpu_info['gpu_name']} ({gpu_info['gpu_memory_gb']}GB)")
-
     if config.deterministic and utils.get_accelerator() == "cuda":
         # Use deterministic algorithms for reproducibility
         os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
diff -urN -x __pycache__ -x '*dist-info' worker/venv/lib/python3.10/site-packages/sd_task/utils.py new_worker/venv/lib/python3.10/site-packages/sd_task/utils.py
--- worker/venv/lib/python3.10/site-packages/sd_task/utils.py	2025-07-07 18:26:32.179630200 +0800
+++ new_worker/venv/lib/python3.10/site-packages/sd_task/utils.py	2025-07-07 18:23:05.923403457 +0800
@@ -7,7 +7,6 @@
 
 
 def get_accelerator():
-    """Detect available accelerator type"""
     if platform.system() == "Darwin":
         try:
             import torch.mps
@@ -27,29 +26,6 @@
     return "cpu"
 
 
-def get_gpu_info():
-    """Get detailed GPU information"""
-    try:
-        import torch
-        if torch.cuda.is_available():
-            gpu_count = torch.cuda.device_count()
-            current_device = torch.cuda.current_device()
-            gpu_name = torch.cuda.get_device_name(current_device)
-            gpu_memory = torch.cuda.get_device_properties(current_device).total_memory
-            gpu_memory_gb = gpu_memory / (1024**3)
-            
-            return {
-                "gpu_count": gpu_count,
-                "current_device": current_device,
-                "gpu_name": gpu_name,
-                "gpu_memory_gb": round(gpu_memory_gb, 2),
-                "compute_capability": torch.cuda.get_device_capability(current_device)
-            }
-    except ImportError:
-        pass
-    
-    return None
-
 def decode_image_dataurl(image_dataurl: str) -> Image.Image:
     image_data = re.sub("^data:image/.+;base64,", "", image_dataurl)
     image = Image.open(BytesIO(base64.b64decode(image_data)))
